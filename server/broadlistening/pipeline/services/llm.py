import logging
import os
import threading

import openai
from dotenv import load_dotenv
from openai import AzureOpenAI, OpenAI
from pydantic import BaseModel
from tenacity import (retry, retry_if_exception_type, stop_after_attempt,
                      wait_exponential)

DOTENV_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../../.env"))
load_dotenv(DOTENV_PATH)

# check env
use_azure = os.getenv("USE_AZURE", "false").lower()
if use_azure == "true":
    if not os.getenv("AZURE_CHATCOMPLETION_ENDPOINT"):
        raise RuntimeError("AZURE_CHATCOMPLETION_ENDPOINT environment variable is not set")
    if not os.getenv("AZURE_CHATCOMPLETION_DEPLOYMENT_NAME"):
        raise RuntimeError("AZURE_CHATCOMPLETION_DEPLOYMENT_NAME environment variable is not set")
    if not os.getenv("AZURE_CHATCOMPLETION_API_KEY"):
        raise RuntimeError("AZURE_CHATCOMPLETION_API_KEY environment variable is not set")
    if not os.getenv("AZURE_CHATCOMPLETION_VERSION"):
        raise RuntimeError("AZURE_CHATCOMPLETION_VERSION environment variable is not set")
    if not os.getenv("AZURE_EMBEDDING_ENDPOINT"):
        raise RuntimeError("AZURE_EMBEDDING_ENDPOINT environment variable is not set")
    if not os.getenv("AZURE_EMBEDDING_API_KEY"):
        raise RuntimeError("AZURE_EMBEDDING_API_KEY environment variable is not set")
    if not os.getenv("AZURE_EMBEDDING_VERSION"):
        raise RuntimeError("AZURE_EMBEDDING_VERSION environment variable is not set")
    if not os.getenv("AZURE_EMBEDDING_DEPLOYMENT_NAME"):
        raise RuntimeError("AZURE_EMBEDDING_DEPLOYMENT_NAME environment variable is not set")


@retry(
    retry=retry_if_exception_type(openai.RateLimitError),
    wait=wait_exponential(multiplier=3, min=3, max=20),
    stop=stop_after_attempt(3),
    reraise=True,
)
def request_to_openai(
    messages: list[dict],
    model: str = "gpt-4",
    is_json: bool = False,
    json_schema: dict | type[BaseModel] | None = None,
) -> str:
    openai.api_type = "openai"

    try:
        if isinstance(json_schema, type) and issubclass(json_schema, BaseModel):
            # Use beta.chat.completions.create for Pydantic BaseModel
            response = openai.beta.chat.completions.parse(
                model=model,
                messages=messages,
                temperature=0,
                n=1,
                seed=0,
                response_format=json_schema,
                timeout=30,
            )
            return response.choices[0].message.content

        else:
            response_format = None
            if is_json:
                response_format = {"type": "json_object"}
            if json_schema:  # ä¸¡æ–¹æœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ãŸã‚‰ã€json_schemaã‚’å„ªå…ˆ
                response_format = json_schema

            payload = {
                "model": model,
                "messages": messages,
                "temperature": 0,
                "n": 1,
                "seed": 0,
                "timeout": 30,
            }
            if response_format:
                payload["response_format"] = response_format

            response = openai.chat.completions.create(**payload)

            return response.choices[0].message.content
    except openai.RateLimitError as e:
        logging.warning(f"OpenAI API rate limit hit: {e}")
        raise
    except openai.AuthenticationError as e:
        logging.error(f"OpenAI API authentication error: {str(e)}")
        raise
    except openai.BadRequestError as e:
        logging.error(f"OpenAI API bad request error: {str(e)}")
        raise


@retry(
    retry=retry_if_exception_type(openai.RateLimitError),
    wait=wait_exponential(multiplier=1, min=2, max=20),
    stop=stop_after_attempt(3),
    reraise=True,
)
def request_to_azure_chatcompletion(
    messages: list[dict],
    is_json: bool = False,
    json_schema: dict | type[BaseModel] | None = None,
) -> str:
    azure_endpoint = os.getenv("AZURE_CHATCOMPLETION_ENDPOINT")
    deployment = os.getenv("AZURE_CHATCOMPLETION_DEPLOYMENT_NAME")
    api_key = os.getenv("AZURE_CHATCOMPLETION_API_KEY")
    api_version = os.getenv("AZURE_CHATCOMPLETION_VERSION")

    client = AzureOpenAI(
        api_version=api_version,
        azure_endpoint=azure_endpoint,
        api_key=api_key,
    )
    # Set response format based on parameters

    try:
        if isinstance(json_schema, type) and issubclass(json_schema, BaseModel):
            # Use beta.chat.completions.create for Pydantic BaseModel (Azure)
            response = client.beta.chat.completions.parse(
                model=deployment,
                messages=messages,
                temperature=0,
                n=1,
                seed=0,
                response_format=json_schema,
                timeout=30,
            )
            return response.choices[0].message.parsed.model_dump()
        else:
            response_format = None
            if is_json:
                response_format = {"type": "json_object"}
            if json_schema:  # ä¸¡æ–¹æœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ãŸã‚‰ã€json_schemaã‚’å„ªå…ˆ
                response_format = json_schema

            payload = {
                "model": deployment,
                "messages": messages,
                "temperature": 0,
                "n": 1,
                "seed": 0,
                "timeout": 30,
            }
            if response_format:
                payload["response_format"] = response_format

            response = client.chat.completions.create(**payload)

            return response.choices[0].message.content
    except openai.RateLimitError as e:
        logging.warning(f"OpenAI API rate limit hit: {e}")
        raise
    except openai.AuthenticationError as e:
        logging.error(f"OpenAI API authentication error: {str(e)}")
        raise
    except openai.BadRequestError as e:
        logging.error(f"OpenAI API bad request error: {str(e)}")
        raise


def request_to_local_llm(
    messages: list[dict],
    model: str,
    is_json: bool = False,
    json_schema: dict | type[BaseModel] | None = None,
    address: str = "localhost:11434",
) -> str:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaã‚„LM Studioï¼‰ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹é–¢æ•°

    OpenAIäº’æ›APIã‚’ä½¿ç”¨ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã¾ã™ã€‚

    Args:
        messages: ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        is_json: JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¦æ±‚ã™ã‚‹ã‹ã©ã†ã‹
        json_schema: JSONã‚¹ã‚­ãƒ¼ãƒï¼ˆPydanticãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯è¾æ›¸ï¼‰
        address: ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆä¾‹: 127.0.0.1:1234ï¼‰

    Returns:
        LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    try:
        if ":" in address:
            host, port_str = address.split(":")
            port = int(port_str)
        else:
            host = address
            port = 11434  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ
    except ValueError:
        logging.warning(f"Invalid address format: {address}, using default")
        host = "localhost"
        port = 11434

    base_url = f"http://{host}:{port}/v1"

    try:
        client = OpenAI(
            base_url=base_url,
            api_key="not-needed",  # Ollamaã¨LM Studioã¯èªè¨¼ä¸è¦
        )

        response_format = None
        if is_json:
            response_format = {"type": "json_object"}
        if json_schema and isinstance(json_schema, dict):
            response_format = json_schema
        if json_schema and isinstance(json_schema, type) and issubclass(json_schema, BaseModel):
            response_format = {
                "type": "json_schema",
                "json_schema": {
                    "name": json_schema.__name__,
                    "strict": True,  # â† ã‚¹ã‚­ãƒ¼ãƒé€¸è„±ã‚’å¼¾ã
                    "schema": json_schema.schema(),
                },
            }

        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0,
            "n": 1,
            "seed": 0,
            "timeout": 30,
        }

        if response_format:
            payload["response_format"] = response_format

        response = client.chat.completions.create(**payload)

        return response.choices[0].message.content
    except Exception as e:
        logging.error(
            f"LocalLLM API error: {e}, model:{model}, address:{address}, is_json:{is_json}, json_schema:{json_schema}, response_format:{response_format}"
        )
        raise


def request_to_chat_openai(
    messages: list[dict],
    model: str = "gpt-4o",
    is_json: bool = False,
    json_schema: dict | type[BaseModel] | None = None,
    provider: str = "openai",
    local_llm_address: str | None = None,
) -> str:
    if provider == "azure":
        return request_to_azure_chatcompletion(messages, is_json, json_schema)
    elif provider == "openai":
        return request_to_openai(messages, model, is_json, json_schema)
    elif provider == "openrouter":
        raise NotImplementedError("OpenRouter support is not implemented yet")
    elif provider == "local":
        address = local_llm_address or "localhost:11434"
        return request_to_local_llm(messages, model, is_json, json_schema, address)
    else:
        raise ValueError(f"Unknown provider: {provider}")


EMBDDING_MODELS = [
    "text-embedding-3-large",
    "text-embedding-3-small",
]


def _validate_model(model):
    if model not in EMBDDING_MODELS:
        raise RuntimeError(f"Invalid embedding model: {model}, available models: {EMBDDING_MODELS}")


def request_to_local_llm_embed(args, model, address="localhost:11434"):
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaã‚„LM Studioï¼‰ã‚’ä½¿ç”¨ã—ã¦åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°

    OpenAIäº’æ›APIã‚’ä½¿ç”¨ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã—ã¾ã™ã€‚

    Args:
        args: åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        address: ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆä¾‹: 127.0.0.1:1234ï¼‰

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    try:
        if ":" in address:
            host, port_str = address.split(":")
            port = int(port_str)
        else:
            host = address
            port = 11434  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ
    except ValueError:
        logging.warning(f"Invalid address format: {address}, using default")
        host = "localhost"
        port = 11434

    base_url = f"http://{host}:{port}/v1"

    try:
        client = OpenAI(
            base_url=base_url,
            api_key="not-needed",  # Ollamaã¨LM Studioã¯èªè¨¼ä¸è¦
        )

        response = client.embeddings.create(input=args, model=model)
        embeds = [item.embedding for item in response.data]
        return embeds
    except Exception as e:
        logging.error(f"LocalLLM embedding API error: {e}")
        logging.warning("Falling back to local embedding")
        return request_to_local_embed(args)


def request_to_embed(args, model, is_embedded_at_local=False, provider="openai", local_llm_address: str | None = None, model_name=None):
    print(f"provider={provider}")
    print(f"is_embedded_at_local={is_embedded_at_local}")
    print(f"model_name={model_name}")
    print(f"model={model}")
    
    if is_embedded_at_local:
        return request_to_local_embed(args, model_name or model)
    if provider == "azure":
        return request_to_azure_embed(args, model)
    elif provider == "openai":
        _validate_model(model)
        client = OpenAI()
        response = client.embeddings.create(input=args, model=model)
        embeds = [item.embedding for item in response.data]
        return embeds
    elif provider == "openrouter":
        raise NotImplementedError("OpenRouter embedding support is not implemented yet")
    elif provider == "local":
        address = local_llm_address or "localhost:11434"
        return request_to_local_llm_embed(args, model, address)
    else:
        raise ValueError(f"Unknown provider: {provider}")


def request_to_azure_embed(args, model):
    azure_endpoint = os.getenv("AZURE_EMBEDDING_ENDPOINT")
    api_key = os.getenv("AZURE_EMBEDDING_API_KEY")
    api_version = os.getenv("AZURE_EMBEDDING_VERSION")
    deployment = os.getenv("AZURE_EMBEDDING_DEPLOYMENT_NAME")

    client = AzureOpenAI(
        api_version=api_version,
        azure_endpoint=azure_endpoint,
        api_key=api_key,
    )

    response = client.embeddings.create(input=args, model=deployment)
    return [item.embedding for item in response.data]


__local_emb_models = {} 
__local_emb_model_loading_lock = threading.Lock()


def request_to_local_embed(args, model_name="paraphrase-multilingual-mpnet-base-v2"):
    global __local_emb_models
    print(f"model_name={model_name}")

    with __local_emb_model_loading_lock:
        if model_name not in __local_emb_models:
            import torch
            from sentence_transformers import SentenceTransformer

            print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {model_name}")
            model = SentenceTransformer(model_name, trust_remote_code=True)

            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()  # âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
                    print("ğŸš€ GPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
                    model = model.to("cuda")
                except torch.cuda.OutOfMemoryError:
                    print("âš ï¸ GPUãƒ¡ãƒ¢ãƒªä¸è¶³: CPUãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
                    model = model.to("cpu")
            else:
                print("âš™ï¸ CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
                model = model.to("cpu")

            __local_emb_models[model_name] = model

    model = __local_emb_models[model_name]

    # âœ… RoSEttaç”¨ã®queryãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹å‡¦ç†
    if model_name == "pkshatech/RoSEtta-base-ja":
        args = [f"query: {text}" for text in args]

    return model.encode(args, convert_to_numpy=True).tolist()


def _test():
    # messages = [
    #     {"role": "system", "content": "è‹±è¨³ã›ã‚ˆ"},
    #     {"role": "user", "content": "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™"},
    # ]
    # response = request_to_chat_openai(messages=messages, model="gpt-4o", is_json=False)
    # print(response)
    # print(request_to_embed("Hello", "text-embedding-3-large"))
    print(request_to_azure_embed("Hello", "text-embedding-3-large"))


def _local_emb_test():
    data = [
        # æ–™ç†é–¢é€£ã®ã‚°ãƒ«ãƒ¼ãƒ—
        "ãƒˆãƒãƒˆã‚½ãƒ¼ã‚¹ã®ãƒ‘ã‚¹ã‚¿ã‚’ä½œã‚‹ã®ãŒå¥½ãã§ã™",
        "ç§ã¯ã‚¤ã‚¿ãƒªã‚¢ãƒ³ã®æ–™ç†ãŒå¾—æ„ã§ã™",
        "ã‚¹ãƒ‘ã‚²ãƒƒãƒ†ã‚£ã‚«ãƒ«ãƒœãƒŠãƒ¼ãƒ©ã¯ç°¡å˜ã«ãŠã„ã—ãä½œã‚Œã¾ã™",
        # å¤©æ°—é–¢é€£ã®ã‚°ãƒ«ãƒ¼ãƒ—
        "ä»Šæ—¥ã¯æ™´ã‚Œã¦æ°—æŒã¡ãŒã„ã„å¤©æ°—ã§ã™",
        "æ˜æ—¥ã®å¤©æ°—äºˆå ±ã§ã¯é›¨ãŒé™ã‚‹ã‚ˆã†ã§ã™",
        "é€±æœ«ã¯å¤©æ°—ãŒè‰¯ããªã‚Šãã†ã§å¤–å‡ºã™ã‚‹ã®ã«æœ€é©ã§ã™",
        # æŠ€è¡“é–¢é€£ã®ã‚°ãƒ«ãƒ¼ãƒ—
        "æ–°ã—ã„ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã¯å‡¦ç†é€Ÿåº¦ãŒé€Ÿããªã‚Šã¾ã—ãŸ",
        "æœ€æ–°ã®ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã¯ãƒãƒƒãƒ†ãƒªãƒ¼æŒã¡ãŒè‰¯ã„ã§ã™",
        "ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³ã®éŸ³è³ªãŒå‘ä¸Šã—ã¦ã„ã¾ã™",
        # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒˆãƒ”ãƒƒã‚¯ï¼ˆç›¸é–¢ãŒä½ã„ã¯ãšï¼‰
        "çŒ«ã¯å¯æ„›ã„å‹•ç‰©ã§ã™",
        "ãƒãƒ£ãƒ¼ãƒãƒ³ã¯ç°¡å˜ã«ä½œã‚Œã‚‹æ–™ç†ã§ã™",
        "å›³æ›¸é¤¨ã§æœ¬ã‚’å€Ÿã‚Šã¦ãã¾ã—ãŸ",
    ]
    emb = request_to_local_embed(data)
    print(emb)

    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—ã®å‡ºåŠ›
    from sklearn.metrics.pairwise import cosine_similarity

    cos_sim = cosine_similarity(emb)
    print(cos_sim)


def _jsonschema_test():
    # JSON schema request example
    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "TranslationResponseModel",
            "schema": {
                "type": "object",
                "properties": {
                    "translation": {"type": "string", "description": "è‹±è¨³çµæœ"},
                    "politeness": {"type": "string", "description": "ä¸å¯§ã•ã®ãƒ¬ãƒ™ãƒ«ï¼ˆä¾‹: casual, polite, honorificï¼‰"},
                },
                "required": ["translation", "politeness"],
            },
        },
    }

    messages = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯ç¿»è¨³è€…ã§ã™ã€‚æ—¥æœ¬èªã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ç¿»è¨³ã¨ä¸å¯§ã•ã®ãƒ¬ãƒ™ãƒ«ã‚’JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚",
        },
        {"role": "user", "content": "ã“ã‚Œã¯ç´ æ™´ã‚‰ã—ã„æ—¥ã§ã™ã€‚"},
    ]

    response = request_to_chat_openai(messages=messages, model="gpt-4o", json_schema=response_format)
    print("JSON Schema response example:")
    print(response)


def _basemodel_test():
    # pydanticã®BaseModelã‚’ä½¿ã£ã¦OpenAI APIã«ã‚¹ã‚­ãƒ¼ãƒã‚’æŒ‡å®šã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã™ã‚‹ãƒ†ã‚¹ãƒˆ
    from pydantic import BaseModel, Field

    class CalendarEvent(BaseModel):
        name: str = Field(..., description="ã‚¤ãƒ™ãƒ³ãƒˆå")
        date: str = Field(..., description="æ—¥ä»˜")
        participants: list[str] = Field(..., description="å‚åŠ è€…")

    messages = [
        {"role": "system", "content": "Extract the event information."},
        {"role": "user", "content": "Alice and Bob are going to a science fair on Friday."},
    ]

    response = request_to_chat_openai(messages=messages, model="gpt-4o", json_schema=CalendarEvent)

    print("Pydantic(BaseModel) schema response example:")
    print(response)


def _local_llm_test():
    # ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹ãƒ†ã‚¹ãƒˆ
    messages = [
        {"role": "system", "content": "Translate the following text to English."},
        {"role": "user", "content": "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™"},
    ]
    response = request_to_local_llm(messages=messages, model="llama-3-elyza-jp-8b", address="localhost:1234")
    print("Local LLM response example:")
    print(response)


if __name__ == "__main__":
    # _test()
    # _test()
    # _jsonschema_test()
    # _basemodel_test()
    # _local_emb_test()
    # _local_llm_test()
    pass
